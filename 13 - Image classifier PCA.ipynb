{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from shutil import copy\n",
    "import itertools \n",
    "import operator\n",
    "from PIL import Image\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classes_list = ['Animals',\n",
    "                'Lifts',\n",
    "                'Other',\n",
    "                'People',\n",
    "                'Summer activity',\n",
    "                'Summer landscape',\n",
    "                'Winter activity',\n",
    "                'Winter landscape']\n",
    "\n",
    "# training data\n",
    "        \n",
    "train = []\n",
    "\n",
    "for i in range(0, len(classes_list)):\n",
    "    images_class = []\n",
    "    path = 'image_classifier/training_data/' + str(classes_list[i])\n",
    "    images_class = [[image, int(image[0:-8]), i, imresize(np.array(Image.open(path + '/' + image)), (224, 224)).reshape(1, -1)[0]] for image in listdir(path) if isfile(join(path, image))]\n",
    "    train = train + images_class\n",
    "\n",
    "X_train = np.asarray([x[3] for x in train])\n",
    "Y_train = np.asarray([x[2] for x in train])\n",
    "\n",
    "del images_class\n",
    "del train\n",
    "\n",
    "# validation data\n",
    "\n",
    "validation = []\n",
    "\n",
    "for i in range(0, len(classes_list)):\n",
    "    images_class = []\n",
    "    path = 'image_classifier/validation_data/' + str(classes_list[i])\n",
    "    images_class = [[image, int(image[0:-8]), i, imresize(np.array(Image.open(path + '/' + image)), (224, 224)).reshape(1, -1)[0]] for image in listdir(path) if isfile(join(path, image))]\n",
    "    validation = validation + images_class\n",
    "\n",
    "X_val = np.asarray([x[3] for x in validation])\n",
    "Y_val = np.asarray([x[2] for x in validation])\n",
    "\n",
    "del images_class\n",
    "del validation\n",
    "\n",
    "# test data\n",
    "\n",
    "test = []\n",
    "\n",
    "for i in range(0, len(classes_list)):\n",
    "    images_class = []\n",
    "    path = 'image_classifier/test_data/' + str(classes_list[i])\n",
    "    images_class = [[image, int(image[0:-8]), i, imresize(np.array(Image.open(path + '/' + image)), (224, 224)).reshape(1, -1)[0]] for image in listdir(path) if isfile(join(path, image))]\n",
    "    test = test + images_class\n",
    "    \n",
    "X_test = np.asarray([x[3] for x in test])\n",
    "Y_test = np.asarray([x[2] for x in test])\n",
    "\n",
    "del images_class\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a standardscaler with the partial_fit method to make sure the features have a mean of zero\n",
    "# and standard deviation of one\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "n = X_train.shape[0]\n",
    "batch_size = 250\n",
    "index = 0\n",
    "\n",
    "# partially fit the scaler on every 1000 samples and update the scaler\n",
    "while index < n:\n",
    "    partial_size = min(batch_size, n - index)\n",
    "    partial_x = X_train[index: index + partial_size]\n",
    "    scaler.partial_fit(partial_x)\n",
    "    index += partial_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training, validation and test set with the standardscaler previously fitted\n",
    "\n",
    "X_train_scaled = np.array([], dtype = 'float32')\n",
    "\n",
    "n = X_train.shape[0]\n",
    "batch_size = 250\n",
    "index = 0\n",
    "\n",
    "# partially transform the data based on the scaler and concatenate the data\n",
    "while index < n:\n",
    "    partial_size = min(batch_size, n - index)\n",
    "    partial_x = X_train[index: index + partial_size]\n",
    "    partial_x = scaler.transform(partial_x).astype('float32')\n",
    "    X_train_scaled = np.vstack([X_train_scaled, partial_x]) if X_train_scaled.size else partial_x\n",
    "    index += partial_size\n",
    "\n",
    "# transform the validation set\n",
    "    \n",
    "X_val_scaled = np.array([], dtype = 'float32')\n",
    "\n",
    "n = X_val.shape[0]\n",
    "batch_size = 250\n",
    "index = 0\n",
    "\n",
    "# partially transform the data based on the scaler and concatenate the data\n",
    "while index < n:\n",
    "    partial_size = min(batch_size, n - index)\n",
    "    partial_x = X_val[index: index + partial_size]\n",
    "    partial_x = scaler.transform(partial_x).astype('float32')\n",
    "    X_val_scaled = np.vstack([X_val_scaled, partial_x]) if X_val_scaled.size else partial_x\n",
    "    index += partial_size\n",
    "\n",
    "# transform the test set\n",
    "\n",
    "X_test_scaled = np.array([], dtype = 'float32')\n",
    "\n",
    "n = X_test.shape[0]\n",
    "batch_size = 250\n",
    "index = 0\n",
    "\n",
    "# partially transform the data based on the scaler and concatenate the data\n",
    "while index < n:\n",
    "    partial_size = min(batch_size, n - index)\n",
    "    partial_x = X_test[index: index + partial_size]\n",
    "    partial_x = scaler.transform(partial_x).astype('float32')\n",
    "    X_test_scaled = np.vstack([X_test_scaled, partial_x]) if X_test_scaled.size else partial_x\n",
    "    index += partial_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the dimensions of the datasets and check if the means are close to zero and the standard deviations close\n",
    "# to one for each feature (to simplify: the sum of all means should equal zero en the sum of all standard deviations should\n",
    "# equal the number of features)\n",
    "\n",
    "print('Information about the training set:\\n')\n",
    "print('   - datatype:', X_train_scaled.dtype)\n",
    "print('   - shape of the dataset:', X_train_scaled.shape)\n",
    "print('   - sum of the means of the columns:', round(X_train_scaled.mean(axis = 0).sum(), 2))\n",
    "print('   - sum of the standard deviations of the columns:', round(X_train_scaled.std(axis = 0).sum(), 2))\n",
    "print('\\n')\n",
    "print('Information about the validation set:\\n')\n",
    "print('   - datatype:', X_val_scaled.dtype)\n",
    "print('   - shape of the dataset:', X_val_scaled.shape)\n",
    "print('   - sum of the means of the columns:', round(X_val_scaled.mean(axis = 0).sum(), 2))\n",
    "print('   - sum of the standard deviations of the columns:', round(X_val_scaled.std(axis = 0).sum(), 2))\n",
    "print('\\n')\n",
    "print('Information about the test set:\\n')\n",
    "print('   - datatype:', X_test_scaled.dtype)\n",
    "print('   - shape of the dataset:', X_test_scaled.shape)\n",
    "print('   - sum of the means of the columns:', round(X_test_scaled.mean(axis = 0).sum(), 2))\n",
    "print('   - sum of the standard deviations of the columns:', round(X_test_scaled.std(axis = 0).sum(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use incremental PCA to fit the data and return 100 principal components for visualization purposes,\n",
    "# eventually I'd like to select the principal components that explain 80% of the variance of the features\n",
    "\n",
    "ipca = IncrementalPCA(n_components = 100, whiten = True, batch_size = 100)\n",
    "\n",
    "ipca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components = np.arange(0, 101)\n",
    "cum_var = np.append(0, np.cumsum(ipca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "threshold = np.full((101,), 0.8)\n",
    "\n",
    "plt.title('\\nExplained variance by n principal components\\n', fontsize = 14)\n",
    "plt.plot(components, cum_var, color = '#FF9933', linewidth = 0.75, linestyle = '-')\n",
    "plt.plot(components, threshold, color = 'red', linewidth = 1, linestyle = '--')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.yaxis.grid(color = '#333333', linestyle = '--', linewidth = 0.25)\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "xticks_major = np.linspace(0, 100, 11).astype('int16')\n",
    "ax.set_xticks(xticks_major)\n",
    "ax.set_xticklabels(xticks_major, fontsize = 11)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "yticks_major = np.round(np.linspace(0, 1, 11), 1)\n",
    "yticks_major_str = (yticks_major * 100).astype(int).astype(str).tolist()\n",
    "yticks_labels = [x + ' %' for x in yticks_major_str]\n",
    "ax.set_yticks(yticks_major)\n",
    "ax.set_yticklabels(yticks_labels, fontsize = 11)\n",
    "\n",
    "ax.set_xlabel('Number of principal components', fontsize = 11, labelpad = 10)\n",
    "ax.set_ylabel('% variance of the original features explained', fontsize = 11)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.text(67, 0.7,\n",
    "         r'67 principal components explain'\"\\n\"r'80% of the variance of the features', fontsize = 10, multialignment = 'center',\n",
    "         bbox = dict(boxstyle = 'round4', facecolor = 'white', alpha = 0.5))\n",
    "\n",
    "plt.annotate(\"\",\n",
    "             xy = (66, 0.8),\n",
    "             xytext = (67.75, 0.76),\n",
    "             arrowprops = dict(arrowstyle = \"simple\", facecolor = \"black\"))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "filename = 'results/IPCA_explained_variance.png'  \n",
    "fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply IPCA transformation on the training, validation and test set with\n",
    "# 36 principal components (they explain 80% of the variance of the features)\n",
    "\n",
    "ipca = IncrementalPCA(n_components = 67, whiten = True, batch_size = 1000)\n",
    "\n",
    "ipca.fit(X_train_scaled)\n",
    "\n",
    "print('The variance of the features explained by the 36 principal components is:', \"{0:.2f}%\".format(sum(ipca.explained_variance_ratio_) * 100))\n",
    "\n",
    "ipca_train = ipca.transform(X_train_scaled)\n",
    "ipca_val = ipca.transform(X_val_scaled)\n",
    "ipca_test = ipca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check what happened to the means and standard deviations\n",
    "\n",
    "print('Information about the training set:\\n')\n",
    "print('   - datatype:', ipca_train.dtype)\n",
    "print('   - shape of the dataset:', ipca_train.shape)\n",
    "print('   - sum of the means of the columns:', round(ipca_train.mean(axis = 0).sum(), 2))\n",
    "print('   - sum of the standard deviations of the columns:', round(ipca_train.std(axis = 0).sum(), 2))\n",
    "print('\\n')\n",
    "print('Information about the validation set:\\n')\n",
    "print('   - datatype:', ipca_val.dtype)\n",
    "print('   - shape of the dataset:', ipca_val.shape)\n",
    "print('   - sum of the means of the columns:', round(ipca_val.mean(axis = 0).sum(), 2))\n",
    "print('   - sum of the standard deviations of the columns:', round(ipca_val.std(axis = 0).sum(), 2))\n",
    "print('\\n')\n",
    "print('Information about the test set:\\n')\n",
    "print('   - datatype:', ipca_test.dtype)\n",
    "print('   - shape of the dataset:', ipca_test.shape)\n",
    "print('   - sum of the means of the columns:', round(ipca_test.mean(axis = 0).sum(), 2))\n",
    "print('   - sum of the standard deviations of the columns:', round(ipca_test.std(axis = 0).sum(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform logistic regression with the principal components to classify the pictures into 5 categories\n",
    "\n",
    "# available optimization methods for multinomial logistic regression:\n",
    "# - newton-cg\n",
    "# - sag\n",
    "# - saga\n",
    "# - lbfgs\n",
    "\n",
    "class_weights = {0: 5.6900,\n",
    "                 1: 1.9965,\n",
    "                 2: 1.5850,\n",
    "                 3: 2.6343,\n",
    "                 4: 3.2330,\n",
    "                 5: 1.6119,\n",
    "                 6: 0.6040,\n",
    "                 7: 1.0000}\n",
    "\n",
    "clf = LogisticRegression(multi_class = 'multinomial',\n",
    "                         solver = 'newton-cg',\n",
    "                         C = 1,\n",
    "                         max_iter = 100000,\n",
    "                         class_weight = class_weights,\n",
    "                         random_state = 0).fit(ipca_train, Y_train)\n",
    "\n",
    "# accuracy for the training set\n",
    "Y_train_pred = clf.predict(ipca_train)\n",
    "accuracy_train = accuracy_score(Y_train, Y_train_pred)\n",
    "\n",
    "# accuracy for the validation set\n",
    "Y_val_pred = clf.predict(ipca_val)\n",
    "accuracy_validation = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "print('Accuracy on the training set:', \"{0:.2f}\".format(accuracy_train))\n",
    "print('Accuracy on the validation set:', \"{0:.2f}\".format(accuracy_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "class_weights = {0: 5.6900,\n",
    "                 1: 1.9965,\n",
    "                 2: 1.5850,\n",
    "                 3: 2.6343,\n",
    "                 4: 3.2330,\n",
    "                 5: 1.6119,\n",
    "                 6: 0.6040,\n",
    "                 7: 1.0000}\n",
    "\n",
    "clf = SVC(kernel = 'rbf', C = 0.01, gamma = 'auto', decision_function_shape = 'multi', class_weight = class_weights).fit(ipca_train, Y_train)\n",
    "\n",
    "# accuracy for the training set\n",
    "Y_train_pred = clf.predict(ipca_train)\n",
    "accuracy_train = accuracy_score(Y_train, Y_train_pred)\n",
    "\n",
    "# accuracy for the validation set\n",
    "Y_val_pred = clf.predict(ipca_val)\n",
    "accuracy_validation = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "print('Accuracy on the training set:', \"{0:.2f}\".format(accuracy_train))\n",
    "print('Accuracy on the validation set:', \"{0:.2f}\".format(accuracy_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
