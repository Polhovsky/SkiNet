{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Merge, Dropout\n",
    "from keras.layers import concatenate, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.initializers import glorot_uniform, he_uniform\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "np.random.seed = 0\n",
    "\n",
    "train_data_dir = 'image_classifier/training_data'\n",
    "validation_data_dir = 'image_classifier/validation_data'\n",
    "test_data_dir = 'image_classifier/test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   fill_mode = 'nearest',\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "val_datagen   = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_datagen  = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "training_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                       target_size = (224, 224),\n",
    "                                                       batch_size = batch_size,\n",
    "                                                       class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                       target_size = (224, 224),\n",
    "                                                       batch_size = batch_size,\n",
    "                                                       class_mode = 'categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                  target_size = (224, 224),\n",
    "                                                  batch_size = 32,\n",
    "                                                  class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = VGG16(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = VGG16.get_layer('fc2').output\n",
    "CNN = Dense(8, activation = 'softmax', name = 'softmax', kernel_initializer = glorot_uniform())(CNN)\n",
    "\n",
    "model = Model(inputs = VGG16.input, outputs = CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1: transfer learning\n",
    "for layer in model.layers[:22]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[22:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use default values of the Adam optimizer for transfer learning\n",
    "adam_transfer = Adam(lr = 1e-4, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "#sgd_transfer = SGD(lr = 0.1, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(optimizer = adam_transfer, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weighting = {0: 5.6900,\n",
    "                   1: 1.9965,\n",
    "                   2: 1.5850,\n",
    "                   3: 2.6343,\n",
    "                   4: 3.2330,\n",
    "                   5: 1.6119,\n",
    "                   6: 0.6040,\n",
    "                   7: 1.0000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 epochs\n",
    "\n",
    "history_transfer = model.fit_generator(generator = training_generator,\n",
    "                                       steps_per_epoch = 3000 // batch_size,\n",
    "                                       validation_data = validation_generator,\n",
    "                                       validation_steps = 1000 // batch_size,\n",
    "                                       epochs = 5,\n",
    "                                       class_weight = class_weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # step 2: finetuning\n",
    "# for layer in model.layers[:4]:\n",
    "#     layer.trainable = False\n",
    "# for layer in model.layers[4:]:\n",
    "#     layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2: finetuning\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's use a smaller learning rate for finetuning since the weights are expected to be quite good already \n",
    "adam_finetune = Adam(lr = 1e-6, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "model.compile(optimizer = adam_finetune, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 epochs\n",
    "\n",
    "history_finetune = model.fit_generator(generator = training_generator,\n",
    "                                       steps_per_epoch = 3000 // batch_size,\n",
    "                                       validation_data = validation_generator,\n",
    "                                       validation_steps = 1000 // batch_size,\n",
    "                                       epochs = 15,\n",
    "                                       class_weight = class_weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loss and accuracy of the test set\n",
    "scores = model.evaluate_generator(generator = test_generator, steps = 1000 // batch_size)\n",
    "print('Loss:', \"{0:.3f}\".format(scores[0]))\n",
    "print('Test accuracy:', \"{0:.2f}%\".format(scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('image_classifier_tl_5_ft_15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('image_classifier_tl_5_ft_15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the history files\n",
    "\n",
    "picklehistory = open('history_transfer.p', 'wb')\n",
    "pickle.dump(history_transfer.history, picklehistory, -1)\n",
    "picklehistory.close()\n",
    "\n",
    "picklehistory = open('history_finetune.p', 'wb')\n",
    "pickle.dump(history_finetune.history, picklehistory, -1)\n",
    "picklehistory.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the history files\n",
    "\n",
    "picklehistory = open('history_transfer.p', 'rb')\n",
    "history_transfer.history = pickle.load(picklehistory)\n",
    "picklehistory.close()\n",
    "\n",
    "picklehistory = open('history_finetune.p', 'rb')\n",
    "history_finetune.history = pickle.load(picklehistory)\n",
    "picklehistory.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = list(zip(test_generator.filenames, test_generator.classes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_adj = []\n",
    "\n",
    "for i in range(0, len(test_set)):\n",
    "    img_path = 'image_classifier/test_data/' + test_set[i][0]\n",
    "    img = image.load_img(img_path, target_size = (224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    probs_i = model.predict(x)\n",
    "    pred_i = np.argmax(probs_i)\n",
    "    \n",
    "    test_i = [img_path] + [test_set[i][1]] + probs_i[0].tolist() + [pred_i]\n",
    "    test_set_adj.append(test_i)\n",
    "\n",
    "test_set_df = pd.DataFrame(test_set_adj)\n",
    "test_set_df.columns = ['filename',\n",
    "                       'y_true',\n",
    "                       'prob_0',\n",
    "                       'prob_1',\n",
    "                       'prob_2',\n",
    "                       'prob_3',\n",
    "                       'prob_4',\n",
    "                       'prob_5',\n",
    "                       'prob_6',\n",
    "                       'prob_7',\n",
    "                       'y_hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df['y_hat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_set_df['y_true'], test_set_df['y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_df.to_csv('test_set_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict for all the pictures\n",
    "\n",
    "account_data_01 = pd.read_csv('results/dataset_analysis.csv', low_memory = False)\n",
    "\n",
    "image_ids_train = pd.read_csv('results/image_ids_train.csv', low_memory = False)\n",
    "image_ids_val = pd.read_csv('results/image_ids_val.csv', low_memory = False)\n",
    "image_ids_test = pd.read_csv('results/image_ids_test.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_path_from_train_df(image_id, label):\n",
    "    image_path = 'training_data/' + str(label) + '/' + str(image_id) + '_256.jpg'\n",
    "    return image_path\n",
    "\n",
    "def file_path_from_validation_df(image_id, label):\n",
    "    image_path = 'validation_data/' + str(label) + '/' + str(image_id) + '_256.jpg'\n",
    "    return image_path\n",
    "\n",
    "def file_path_from_test_df(image_id, label):\n",
    "    image_path = 'test_data/' + str(label) + '/' + str(image_id) + '_256.jpg'\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = account_data_01.merge(image_ids_train, on = 'image_id', how = 'inner')\n",
    "train_df = train_df[['image_id', 'likes_groups']]\n",
    "train_df['label'] = train_df.likes_groups.apply(lambda x: str(x)[0])\n",
    "train_df['path'] = train_df.apply(lambda x: file_path_from_train_df(x['image_id'], x['label']), axis = 1)\n",
    "\n",
    "validation_df = account_data_01.merge(image_ids_val, on = 'image_id', how = 'inner')\n",
    "validation_df = validation_df[['image_id', 'likes_groups']]\n",
    "validation_df['label'] = validation_df.likes_groups.apply(lambda x: str(x)[0])\n",
    "validation_df['path'] = validation_df.apply(lambda x: file_path_from_validation_df(x['image_id'], x['label']), axis = 1)\n",
    "\n",
    "test_df = account_data_01.merge(image_ids_test, on = 'image_id', how = 'inner')\n",
    "test_df = test_df[['image_id', 'likes_groups']]\n",
    "test_df['label'] = test_df.likes_groups.apply(lambda x: str(x)[0])\n",
    "test_df['path'] = test_df.apply(lambda x: file_path_from_test_df(x['image_id'], x['label']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_paths = list(zip(train_df['image_id'], train_df['path']))\n",
    "validation_paths = list(zip(validation_df['image_id'], validation_df['path']))\n",
    "test_paths = list(zip(test_df['image_id'], test_df['path']))\n",
    "\n",
    "paths = train_paths + validation_paths + test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "\n",
    "for i in range(0, len(paths)):\n",
    "    img_path = paths[i][1]\n",
    "    img = image.load_img(img_path, target_size = (224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)           \n",
    "    \n",
    "    probs_i = model.predict(x)\n",
    "    pred_i = np.argmax(probs_i)\n",
    "    \n",
    "    record = [paths[i][0]] + [img_path] + probs_i[0].tolist() + [pred_i]\n",
    "    total.append(record)\n",
    "    \n",
    "    if (i > 0) & (i % 5000 == 0):\n",
    "        print(i, 'pictures processed')\n",
    "\n",
    "total_df = pd.DataFrame(total)\n",
    "total_df.columns = ['image_id',\n",
    "                    'filename',\n",
    "                    'prob_0',\n",
    "                    'prob_1',\n",
    "                    'prob_2',\n",
    "                    'prob_3',\n",
    "                    'prob_4',\n",
    "                    'prob_5',\n",
    "                    'prob_6',\n",
    "                    'prob_7',\n",
    "                    'y_hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df.to_csv('total_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
