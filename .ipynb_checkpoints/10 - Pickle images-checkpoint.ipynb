{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from keras.utils import np_utils\n",
    "\n",
    "account_data_01 = pd.read_csv('results/dataset_analysis.csv', low_memory = False)\n",
    "clusters = pd.read_csv('results/clusters.csv', low_memory = False)\n",
    "account_data_02 = account_data_01.merge(clusters, on = 'image_id', how = 'inner')\n",
    "\n",
    "print('Number of samples in account_data_01:', account_data_01.shape[0])\n",
    "print('Number of samples in account_data_02:', account_data_02.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = account_data_02.values.tolist()\n",
    "\n",
    "picture_size = 256\n",
    "\n",
    "picture_batches = [0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, len(image_list)]\n",
    "\n",
    "for i in range(1, len(picture_batches)):\n",
    "\n",
    "    X = np.array([])\n",
    "    Y = np.array([], dtype = 'int32')\n",
    "    clusters = np.array([], dtype = 'int32')\n",
    "    image_ids = np.array([], dtype = 'int32')\n",
    "\n",
    "    for j in range(picture_batches[i - 1], picture_batches[i]):\n",
    "\n",
    "        # define the location of the pictures of a particular resort\n",
    "        folder_path = 'images/{}/'.format(str(image_list[j][4]))\n",
    "\n",
    "        # import the picture if it exists\n",
    "        if os.path.isfile(folder_path + '/crop/' + str(image_list[j][5]) + '_' + str(picture_size) + '.jpg'):\n",
    "            img = Image.open(folder_path + '/crop/' + str(image_list[j][5]) + '_' + str(picture_size) + '.jpg')\n",
    "\n",
    "            # convert picture to an array, reshape to the right dimensions for input in Keras and normalize the values\n",
    "            image = np.array(img, dtype = 'uint8').reshape(1, picture_size, picture_size, 3)\n",
    "\n",
    "            del img\n",
    "\n",
    "            # add the pictures together in a matrix\n",
    "            X = np.vstack([X, image]) if X.size else image\n",
    "\n",
    "            del image\n",
    "\n",
    "            # get the target variable\n",
    "            likes_group = (int(image_list[j][0][0]))\n",
    "            Y = np.append([Y], likes_group)\n",
    "\n",
    "            del likes_group\n",
    "\n",
    "            # save the cluster the picture belongs to\n",
    "            cluster = int(image_list[j][-1])\n",
    "            clusters = np.append([clusters], cluster)\n",
    "\n",
    "            del cluster\n",
    "\n",
    "            # save the image_id as a reference\n",
    "            image_id = int(image_list[j][5])\n",
    "            image_ids = np.append([image_ids], image_id)\n",
    "\n",
    "            del image_id\n",
    "\n",
    "            # print the progress of the import\n",
    "            if (j > 0) and (j%1000) == 0:\n",
    "                print(j, 'images have been processed')\n",
    "\n",
    "    # make image_ids, clusters and Y 2-dimensional\n",
    "    image_ids = image_ids.reshape(-1, 1)\n",
    "    clusters = clusters.reshape(-1, 1)\n",
    "    Y = Y.reshape(-1, 1)\n",
    "\n",
    "    # np.utils.to_categorical is used to convert array of labeled data (from 0 to nb_classes-1) to one-hot vector\n",
    "    Y_one_hot_encoded = np_utils.to_categorical(Y)[:, 1:6]\n",
    "    \n",
    "    # save datasets as pickle file\n",
    "    pickleX = open('results/CNN_X_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'wb')\n",
    "    pickle.dump(X, pickleX, -1)\n",
    "    pickleX.close()\n",
    "\n",
    "    pickleY = open('results/CNN_Y_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'wb')\n",
    "    pickle.dump(Y, pickleY, -1)\n",
    "    pickleY.close()\n",
    "\n",
    "    pickleY_one_hot_encoded = open('results/CNN_Y_one_hot_encoded_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'wb')\n",
    "    pickle.dump(Y_one_hot_encoded, pickleY_one_hot_encoded, -1)\n",
    "    pickleY_one_hot_encoded.close()\n",
    "\n",
    "    pickleimage_ids = open('results/CNN_image_ids_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'wb')\n",
    "    pickle.dump(image_ids, pickleimage_ids, -1)\n",
    "    pickleimage_ids.close()\n",
    "\n",
    "    pickleclusters = open('results/CNN_clusters_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'wb')\n",
    "    pickle.dump(clusters, pickleclusters, -1)\n",
    "    pickleclusters.close()\n",
    "    \n",
    "    del X\n",
    "    del Y\n",
    "    del Y_one_hot_encoded\n",
    "    del image_ids\n",
    "    del clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = account_data_02.values.tolist()\n",
    "\n",
    "X = np.array([])\n",
    "Y = np.array([], dtype = 'int32')\n",
    "Y_one_hot_encoded = np.array([], dtype = 'int32')\n",
    "clusters = np.array([], dtype = 'int32')\n",
    "image_ids = np.array([], dtype = 'int32')\n",
    "\n",
    "picture_batches = [0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, len(image_list)]\n",
    "\n",
    "for i in range(1, len(picture_batches)):\n",
    "\n",
    "    pickleX = open('results/CNN_X_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'rb')\n",
    "    X_temp = pickle.load(pickleX)\n",
    "    pickleX.close()\n",
    "\n",
    "    pickleY = open('results/CNN_Y_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'rb')\n",
    "    Y_temp = pickle.load(pickleY)\n",
    "    pickleY.close()\n",
    "\n",
    "    pickleY_one_hot_encoded = open('results/CNN_Y_one_hot_encoded_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'rb')\n",
    "    Y_one_hot_encoded_temp = pickle.load(pickleY_one_hot_encoded)\n",
    "    pickleY_one_hot_encoded.close()\n",
    "\n",
    "    pickleimage_ids = open('results/CNN_image_ids_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'rb')\n",
    "    image_ids_temp = pickle.load(pickleimage_ids)\n",
    "    pickleimage_ids.close()\n",
    "\n",
    "    pickleclusters = open('results/CNN_clusters_' + str(picture_size) + '_' + str(picture_batches[i - 1]) + '_' + str(picture_batches[i]) + '.p', 'rb')\n",
    "    clusters_temp = pickle.load(pickleclusters)\n",
    "    pickleclusters.close()\n",
    "                          \n",
    "    X = np.vstack([X, X_temp]) if X.size else X_temp\n",
    "    \n",
    "    del X_temp\n",
    "\n",
    "    Y = np.append([Y], Y_temp)\n",
    "    \n",
    "    del Y_temp\n",
    "\n",
    "    Y_one_hot_encoded = np.vstack([Y_one_hot_encoded, Y_one_hot_encoded_temp]) if Y_one_hot_encoded.size else Y_one_hot_encoded_temp\n",
    "    \n",
    "    del Y_one_hot_encoded_temp\n",
    "\n",
    "    image_ids = np.append([image_ids], image_ids_temp)\n",
    "\n",
    "    del image_ids_temp\n",
    "    \n",
    "    clusters = np.append([clusters], clusters_temp)\n",
    "    \n",
    "    del clusters_temp\n",
    "\n",
    "Y = Y.reshape(-1, 1)\n",
    "image_ids = image_ids.reshape(-1, 1)\n",
    "clusters = clusters.reshape(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
